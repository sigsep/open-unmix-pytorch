<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>openunmix.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>openunmix.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L0-L304" class="git-link">Browse git</a>
</summary>
<pre><code class="python">from typing import Optional, Union

import torch
import os
import numpy as np
import torchaudio
import warnings
from pathlib import Path
from contextlib import redirect_stderr
import io
import json

import openunmix
from openunmix import model


def bandwidth_to_max_bin(rate: float, n_fft: int, bandwidth: float) -&gt; np.ndarray:
    &#34;&#34;&#34;Convert bandwidth to maximum bin count

    Assuming lapped transforms such as STFT

    Args:
        rate (int): Sample rate
        n_fft (int): FFT length
        bandwidth (float): Target bandwidth in Hz

    Returns:
        np.ndarray: maximum frequency bin
    &#34;&#34;&#34;
    freqs = np.linspace(0, rate / 2, n_fft // 2 + 1, endpoint=True)

    return np.max(np.where(freqs &lt;= bandwidth)[0]) + 1


def save_checkpoint(state: dict, is_best: bool, path: str, target: str):
    &#34;&#34;&#34;Convert bandwidth to maximum bin count

    Assuming lapped transforms such as STFT

    Args:
        state (dict): torch model state dict
        is_best (bool): if current model is about to be saved as best model
        path (str): model path
        target (str): target name
    &#34;&#34;&#34;
    # save full checkpoint including optimizer
    torch.save(state, os.path.join(path, target + &#34;.chkpnt&#34;))
    if is_best:
        # save just the weights
        torch.save(state[&#34;state_dict&#34;], os.path.join(path, target + &#34;.pth&#34;))


class AverageMeter(object):
    &#34;&#34;&#34;Computes and stores the average and current value&#34;&#34;&#34;

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


class EarlyStopping(object):
    &#34;&#34;&#34;Early Stopping Monitor&#34;&#34;&#34;

    def __init__(self, mode=&#34;min&#34;, min_delta=0, patience=10):
        self.mode = mode
        self.min_delta = min_delta
        self.patience = patience
        self.best = None
        self.num_bad_epochs = 0
        self.is_better = None
        self._init_is_better(mode, min_delta)

        if patience == 0:
            self.is_better = lambda a, b: True

    def step(self, metrics):
        if self.best is None:
            self.best = metrics
            return False

        if np.isnan(metrics):
            return True

        if self.is_better(metrics, self.best):
            self.num_bad_epochs = 0
            self.best = metrics
        else:
            self.num_bad_epochs += 1

        if self.num_bad_epochs &gt;= self.patience:
            return True

        return False

    def _init_is_better(self, mode, min_delta):
        if mode not in {&#34;min&#34;, &#34;max&#34;}:
            raise ValueError(&#34;mode &#34; + mode + &#34; is unknown!&#34;)
        if mode == &#34;min&#34;:
            self.is_better = lambda a, best: a &lt; best - min_delta
        if mode == &#34;max&#34;:
            self.is_better = lambda a, best: a &gt; best + min_delta


def load_target_models(targets, model_str_or_path=&#34;umxhq&#34;, device=&#34;cpu&#34;, pretrained=True):
    &#34;&#34;&#34;Core model loader

    target model path can be either &lt;target&gt;.pth, or &lt;target&gt;-sha256.pth
    (as used on torchub)

    The loader either loads the models from a known model string
    as registered in the __init__.py or loads from custom configs.
    &#34;&#34;&#34;
    if isinstance(targets, str):
        targets = [targets]

    model_path = Path(model_str_or_path).expanduser()
    if not model_path.exists():
        # model path does not exist, use pretrained models
        try:
            # disable progress bar
            hub_loader = getattr(openunmix, model_str_or_path + &#34;_spec&#34;)
            err = io.StringIO()
            with redirect_stderr(err):
                return hub_loader(targets=targets, device=device, pretrained=pretrained)
            print(err.getvalue())
        except AttributeError:
            raise NameError(&#34;Model does not exist on torchhub&#34;)
            # assume model is a path to a local model_str_or_path directory
    else:
        models = {}
        for target in targets:
            # load model from disk
            with open(Path(model_path, target + &#34;.json&#34;), &#34;r&#34;) as stream:
                results = json.load(stream)

            target_model_path = next(Path(model_path).glob(&#34;%s*.pth&#34; % target))
            state = torch.load(target_model_path, map_location=device)

            models[target] = model.OpenUnmix(
                nb_bins=results[&#34;args&#34;][&#34;nfft&#34;] // 2 + 1,
                nb_channels=results[&#34;args&#34;][&#34;nb_channels&#34;],
                hidden_size=results[&#34;args&#34;][&#34;hidden_size&#34;],
                max_bin=state[&#34;input_mean&#34;].shape[0],
            )

            if pretrained:
                models[target].load_state_dict(state, strict=False)

            models[target].to(device)
        return models


def load_separator(
    model_str_or_path: str = &#34;umxhq&#34;,
    targets: Optional[list] = None,
    niter: int = 1,
    residual: bool = False,
    wiener_win_len: Optional[int] = 300,
    device: Union[str, torch.device] = &#34;cpu&#34;,
    pretrained: bool = True,
    filterbank: str = &#34;torch&#34;,
):
    &#34;&#34;&#34;Separator loader

    Args:
        model_str_or_path (str): Model name or path to model _parent_ directory
            E.g. The following files are assumed to present when
            loading `model_str_or_path=&#39;mymodel&#39;, targets=[&#39;vocals&#39;]`
            &#39;mymodel/separator.json&#39;, mymodel/vocals.pth&#39;, &#39;mymodel/vocals.json&#39;.
            Defaults to `umxhq`.
        targets (list of str or None): list of target names. When loading a
            pre-trained model, all `targets` can be None as all targets
            will be loaded
        niter (int): Number of EM steps for refining initial estimates
            in a post-processing stage. `--niter 0` skips this step altogether
            (and thus makes separation significantly faster) More iterations
            can get better interference reduction at the price of artifacts.
            Defaults to `1`.
        residual (bool): Computes a residual target, for custom separation
            scenarios when not all targets are available (at the expense
            of slightly less performance). E.g vocal/accompaniment
            Defaults to `False`.
        wiener_win_len (int): The size of the excerpts (number of frames) on
            which to apply filtering independently. This means assuming
            time varying stereo models and localization of sources.
            None means not batching but using the whole signal. It comes at the
            price of a much larger memory usage.
            Defaults to `300`
        device (str): torch device, defaults to `cpu`
        pretrained (bool): determines if loading pre-trained weights
        filterbank (str): filterbank implementation method.
            Supported are `[&#39;torch&#39;, &#39;asteroid&#39;]`. `torch` is about 30% faster
            compared to `asteroid` on large FFT sizes such as 4096. However,
            asteroids stft can be exported to onnx, which makes is practical
            for deployment.
    &#34;&#34;&#34;
    model_path = Path(model_str_or_path).expanduser()

    # when path exists, we assume its a custom model saved locally
    if model_path.exists():
        if targets is None:
            raise UserWarning(&#34;For custom models, please specify the targets&#34;)

        target_models = load_target_models(
            targets=targets, model_str_or_path=model_path, pretrained=pretrained
        )

        with open(Path(model_path, &#34;separator.json&#34;), &#34;r&#34;) as stream:
            enc_conf = json.load(stream)

        separator = model.Separator(
            target_models=target_models,
            niter=niter,
            residual=residual,
            wiener_win_len=wiener_win_len,
            sample_rate=enc_conf[&#34;sample_rate&#34;],
            n_fft=enc_conf[&#34;nfft&#34;],
            n_hop=enc_conf[&#34;nhop&#34;],
            nb_channels=enc_conf[&#34;nb_channels&#34;],
            filterbank=filterbank,
        ).to(device)

    # otherwise we load the separator from torchhub
    else:
        hub_loader = getattr(openunmix, model_str_or_path)
        separator = hub_loader(
            targets=targets,
            device=device,
            pretrained=True,
            niter=niter,
            residual=residual,
            filterbank=filterbank,
        )

    return separator


def preprocess(
    audio: torch.Tensor,
    rate: Optional[float] = None,
    model_rate: Optional[float] = None,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    From an input tensor, convert it to a tensor of shape
    shape=(nb_samples, nb_channels, nb_timesteps). This includes:
    -  if input is 1D, adding the samples and channels dimensions.
    -  if input is 2D
        o and the smallest dimension is 1 or 2, adding the samples one.
        o and all dimensions are &gt; 2, assuming the smallest is the samples
          one, and adding the channel one
    - at the end, if the number of channels is greater than the number
      of time steps, swap those two.
    - resampling to target rate if necessary

    Args:
        audio (Tensor): input waveform
        rate (float): sample rate for the audio
        model_rate (float): sample rate for the model

    Returns:
        Tensor: [shape=(nb_samples, nb_channels=2, nb_timesteps)]
    &#34;&#34;&#34;
    shape = torch.as_tensor(audio.shape, device=audio.device)

    if len(shape) == 1:
        # assuming only time dimension is provided.
        audio = audio[None, None, ...]
    elif len(shape) == 2:
        if shape.min() &lt;= 2:
            # assuming sample dimension is missing
            audio = audio[None, ...]
        else:
            # assuming channel dimension is missing
            audio = audio[:, None, ...]
    if audio.shape[1] &gt; audio.shape[2]:
        # swapping channel and time
        audio = audio.transpose(1, 2)
    if audio.shape[1] &gt; 2:
        warnings.warn(&#34;Channel count &gt; 2!. Only the first two channels &#34; &#34;will be processed!&#34;)
        audio = audio[..., :2]

    if audio.shape[1] == 1:
        # if we have mono, we duplicate it to get stereo
        audio = torch.repeat_interleave(audio, 2, dim=1)

    if rate != model_rate:
        print(&#34;resampling&#34;)
        # we have to resample to model samplerate if needed
        # this makes sure we resample input only once
        resampler = torchaudio.transforms.Resample(
            orig_freq=rate, new_freq=model_rate, resampling_method=&#34;sinc_interpolation&#34;
        ).to(audio.device)
        audio = resampler(audio)
    return audio</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="openunmix.utils.bandwidth_to_max_bin"><code class="name flex">
<span>def <span class="ident">bandwidth_to_max_bin</span></span>(<span>rate: float, n_fft: int, bandwidth: float) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Convert bandwidth to maximum bin count</p>
<p>Assuming lapped transforms such as STFT</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rate</code></strong> :&ensp;<code>int</code></dt>
<dd>Sample rate</dd>
<dt><strong><code>n_fft</code></strong> :&ensp;<code>int</code></dt>
<dd>FFT length</dd>
<dt><strong><code>bandwidth</code></strong> :&ensp;<code>float</code></dt>
<dd>Target bandwidth in Hz</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>maximum frequency bin</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L17-L32" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def bandwidth_to_max_bin(rate: float, n_fft: int, bandwidth: float) -&gt; np.ndarray:
    &#34;&#34;&#34;Convert bandwidth to maximum bin count

    Assuming lapped transforms such as STFT

    Args:
        rate (int): Sample rate
        n_fft (int): FFT length
        bandwidth (float): Target bandwidth in Hz

    Returns:
        np.ndarray: maximum frequency bin
    &#34;&#34;&#34;
    freqs = np.linspace(0, rate / 2, n_fft // 2 + 1, endpoint=True)

    return np.max(np.where(freqs &lt;= bandwidth)[0]) + 1</code></pre>
</details>
</dd>
<dt id="openunmix.utils.load_separator"><code class="name flex">
<span>def <span class="ident">load_separator</span></span>(<span>model_str_or_path: str = 'umxhq', targets: Union[list, NoneType] = None, niter: int = 1, residual: bool = False, wiener_win_len: Union[int, NoneType] = 300, device: Union[str, torch.device] = 'cpu', pretrained: bool = True, filterbank: str = 'torch')</span>
</code></dt>
<dd>
<div class="desc"><p>Separator loader</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_str_or_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Model name or path to model <em>parent</em> directory
E.g. The following files are assumed to present when
loading <code>model_str_or_path='mymodel', targets=['vocals']</code>
'mymodel/separator.json', mymodel/vocals.pth', 'mymodel/vocals.json'.
Defaults to <code>umxhq</code>.</dd>
<dt><strong><code>targets</code></strong> :&ensp;<code>list</code> of <code>str</code> or <code>None</code></dt>
<dd>list of target names. When loading a
pre-trained model, all <code>targets</code> can be None as all targets
will be loaded</dd>
<dt><strong><code>niter</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of EM steps for refining initial estimates
in a post-processing stage. <code>--niter 0</code> skips this step altogether
(and thus makes separation significantly faster) More iterations
can get better interference reduction at the price of artifacts.
Defaults to <code>1</code>.</dd>
<dt><strong><code>residual</code></strong> :&ensp;<code>bool</code></dt>
<dd>Computes a residual target, for custom separation
scenarios when not all targets are available (at the expense
of slightly less performance). E.g vocal/accompaniment
Defaults to <code>False</code>.</dd>
<dt><strong><code>wiener_win_len</code></strong> :&ensp;<code>int</code></dt>
<dd>The size of the excerpts (number of frames) on
which to apply filtering independently. This means assuming
time varying stereo models and localization of sources.
None means not batching but using the whole signal. It comes at the
price of a much larger memory usage.
Defaults to <code>300</code></dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code></dt>
<dd>torch device, defaults to <code>cpu</code></dd>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines if loading pre-trained weights</dd>
<dt><strong><code>filterbank</code></strong> :&ensp;<code>str</code></dt>
<dd>filterbank implementation method.
Supported are <code>['torch', 'asteroid']</code>. <code>torch</code> is about 30% faster
compared to <code>asteroid</code> on large FFT sizes such as 4096. However,
asteroids stft can be exported to onnx, which makes is practical
for deployment.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L164-L246" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def load_separator(
    model_str_or_path: str = &#34;umxhq&#34;,
    targets: Optional[list] = None,
    niter: int = 1,
    residual: bool = False,
    wiener_win_len: Optional[int] = 300,
    device: Union[str, torch.device] = &#34;cpu&#34;,
    pretrained: bool = True,
    filterbank: str = &#34;torch&#34;,
):
    &#34;&#34;&#34;Separator loader

    Args:
        model_str_or_path (str): Model name or path to model _parent_ directory
            E.g. The following files are assumed to present when
            loading `model_str_or_path=&#39;mymodel&#39;, targets=[&#39;vocals&#39;]`
            &#39;mymodel/separator.json&#39;, mymodel/vocals.pth&#39;, &#39;mymodel/vocals.json&#39;.
            Defaults to `umxhq`.
        targets (list of str or None): list of target names. When loading a
            pre-trained model, all `targets` can be None as all targets
            will be loaded
        niter (int): Number of EM steps for refining initial estimates
            in a post-processing stage. `--niter 0` skips this step altogether
            (and thus makes separation significantly faster) More iterations
            can get better interference reduction at the price of artifacts.
            Defaults to `1`.
        residual (bool): Computes a residual target, for custom separation
            scenarios when not all targets are available (at the expense
            of slightly less performance). E.g vocal/accompaniment
            Defaults to `False`.
        wiener_win_len (int): The size of the excerpts (number of frames) on
            which to apply filtering independently. This means assuming
            time varying stereo models and localization of sources.
            None means not batching but using the whole signal. It comes at the
            price of a much larger memory usage.
            Defaults to `300`
        device (str): torch device, defaults to `cpu`
        pretrained (bool): determines if loading pre-trained weights
        filterbank (str): filterbank implementation method.
            Supported are `[&#39;torch&#39;, &#39;asteroid&#39;]`. `torch` is about 30% faster
            compared to `asteroid` on large FFT sizes such as 4096. However,
            asteroids stft can be exported to onnx, which makes is practical
            for deployment.
    &#34;&#34;&#34;
    model_path = Path(model_str_or_path).expanduser()

    # when path exists, we assume its a custom model saved locally
    if model_path.exists():
        if targets is None:
            raise UserWarning(&#34;For custom models, please specify the targets&#34;)

        target_models = load_target_models(
            targets=targets, model_str_or_path=model_path, pretrained=pretrained
        )

        with open(Path(model_path, &#34;separator.json&#34;), &#34;r&#34;) as stream:
            enc_conf = json.load(stream)

        separator = model.Separator(
            target_models=target_models,
            niter=niter,
            residual=residual,
            wiener_win_len=wiener_win_len,
            sample_rate=enc_conf[&#34;sample_rate&#34;],
            n_fft=enc_conf[&#34;nfft&#34;],
            n_hop=enc_conf[&#34;nhop&#34;],
            nb_channels=enc_conf[&#34;nb_channels&#34;],
            filterbank=filterbank,
        ).to(device)

    # otherwise we load the separator from torchhub
    else:
        hub_loader = getattr(openunmix, model_str_or_path)
        separator = hub_loader(
            targets=targets,
            device=device,
            pretrained=True,
            niter=niter,
            residual=residual,
            filterbank=filterbank,
        )

    return separator</code></pre>
</details>
</dd>
<dt id="openunmix.utils.load_target_models"><code class="name flex">
<span>def <span class="ident">load_target_models</span></span>(<span>targets, model_str_or_path='umxhq', device='cpu', pretrained=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Core model loader</p>
<p>target model path can be either <target>.pth, or <target>-sha256.pth
(as used on torchub)</p>
<p>The loader either loads the models from a known model string
as registered in the <strong>init</strong>.py or loads from custom configs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L115-L161" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def load_target_models(targets, model_str_or_path=&#34;umxhq&#34;, device=&#34;cpu&#34;, pretrained=True):
    &#34;&#34;&#34;Core model loader

    target model path can be either &lt;target&gt;.pth, or &lt;target&gt;-sha256.pth
    (as used on torchub)

    The loader either loads the models from a known model string
    as registered in the __init__.py or loads from custom configs.
    &#34;&#34;&#34;
    if isinstance(targets, str):
        targets = [targets]

    model_path = Path(model_str_or_path).expanduser()
    if not model_path.exists():
        # model path does not exist, use pretrained models
        try:
            # disable progress bar
            hub_loader = getattr(openunmix, model_str_or_path + &#34;_spec&#34;)
            err = io.StringIO()
            with redirect_stderr(err):
                return hub_loader(targets=targets, device=device, pretrained=pretrained)
            print(err.getvalue())
        except AttributeError:
            raise NameError(&#34;Model does not exist on torchhub&#34;)
            # assume model is a path to a local model_str_or_path directory
    else:
        models = {}
        for target in targets:
            # load model from disk
            with open(Path(model_path, target + &#34;.json&#34;), &#34;r&#34;) as stream:
                results = json.load(stream)

            target_model_path = next(Path(model_path).glob(&#34;%s*.pth&#34; % target))
            state = torch.load(target_model_path, map_location=device)

            models[target] = model.OpenUnmix(
                nb_bins=results[&#34;args&#34;][&#34;nfft&#34;] // 2 + 1,
                nb_channels=results[&#34;args&#34;][&#34;nb_channels&#34;],
                hidden_size=results[&#34;args&#34;][&#34;hidden_size&#34;],
                max_bin=state[&#34;input_mean&#34;].shape[0],
            )

            if pretrained:
                models[target].load_state_dict(state, strict=False)

            models[target].to(device)
        return models</code></pre>
</details>
</dd>
<dt id="openunmix.utils.preprocess"><code class="name flex">
<span>def <span class="ident">preprocess</span></span>(<span>audio: torch.Tensor, rate: Union[float, NoneType] = None, model_rate: Union[float, NoneType] = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>From an input tensor, convert it to a tensor of shape
shape=(nb_samples, nb_channels, nb_timesteps). This includes:
-
if input is 1D, adding the samples and channels dimensions.
-
if input is 2D
o and the smallest dimension is 1 or 2, adding the samples one.
o and all dimensions are &gt; 2, assuming the smallest is the samples
one, and adding the channel one
- at the end, if the number of channels is greater than the number
of time steps, swap those two.
- resampling to target rate if necessary</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>audio</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>input waveform</dd>
<dt><strong><code>rate</code></strong> :&ensp;<code>float</code></dt>
<dd>sample rate for the audio</dd>
<dt><strong><code>model_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>sample rate for the model</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>[shape=(nb_samples, nb_channels=2, nb_timesteps)]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L249-L305" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def preprocess(
    audio: torch.Tensor,
    rate: Optional[float] = None,
    model_rate: Optional[float] = None,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    From an input tensor, convert it to a tensor of shape
    shape=(nb_samples, nb_channels, nb_timesteps). This includes:
    -  if input is 1D, adding the samples and channels dimensions.
    -  if input is 2D
        o and the smallest dimension is 1 or 2, adding the samples one.
        o and all dimensions are &gt; 2, assuming the smallest is the samples
          one, and adding the channel one
    - at the end, if the number of channels is greater than the number
      of time steps, swap those two.
    - resampling to target rate if necessary

    Args:
        audio (Tensor): input waveform
        rate (float): sample rate for the audio
        model_rate (float): sample rate for the model

    Returns:
        Tensor: [shape=(nb_samples, nb_channels=2, nb_timesteps)]
    &#34;&#34;&#34;
    shape = torch.as_tensor(audio.shape, device=audio.device)

    if len(shape) == 1:
        # assuming only time dimension is provided.
        audio = audio[None, None, ...]
    elif len(shape) == 2:
        if shape.min() &lt;= 2:
            # assuming sample dimension is missing
            audio = audio[None, ...]
        else:
            # assuming channel dimension is missing
            audio = audio[:, None, ...]
    if audio.shape[1] &gt; audio.shape[2]:
        # swapping channel and time
        audio = audio.transpose(1, 2)
    if audio.shape[1] &gt; 2:
        warnings.warn(&#34;Channel count &gt; 2!. Only the first two channels &#34; &#34;will be processed!&#34;)
        audio = audio[..., :2]

    if audio.shape[1] == 1:
        # if we have mono, we duplicate it to get stereo
        audio = torch.repeat_interleave(audio, 2, dim=1)

    if rate != model_rate:
        print(&#34;resampling&#34;)
        # we have to resample to model samplerate if needed
        # this makes sure we resample input only once
        resampler = torchaudio.transforms.Resample(
            orig_freq=rate, new_freq=model_rate, resampling_method=&#34;sinc_interpolation&#34;
        ).to(audio.device)
        audio = resampler(audio)
    return audio</code></pre>
</details>
</dd>
<dt id="openunmix.utils.save_checkpoint"><code class="name flex">
<span>def <span class="ident">save_checkpoint</span></span>(<span>state: dict, is_best: bool, path: str, target: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert bandwidth to maximum bin count</p>
<p>Assuming lapped transforms such as STFT</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>dict</code></dt>
<dd>torch model state dict</dd>
<dt><strong><code>is_best</code></strong> :&ensp;<code>bool</code></dt>
<dd>if current model is about to be saved as best model</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>model path</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>str</code></dt>
<dd>target name</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L35-L50" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def save_checkpoint(state: dict, is_best: bool, path: str, target: str):
    &#34;&#34;&#34;Convert bandwidth to maximum bin count

    Assuming lapped transforms such as STFT

    Args:
        state (dict): torch model state dict
        is_best (bool): if current model is about to be saved as best model
        path (str): model path
        target (str): target name
    &#34;&#34;&#34;
    # save full checkpoint including optimizer
    torch.save(state, os.path.join(path, target + &#34;.chkpnt&#34;))
    if is_best:
        # save just the weights
        torch.save(state[&#34;state_dict&#34;], os.path.join(path, target + &#34;.pth&#34;))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="openunmix.utils.AverageMeter"><code class="flex name class">
<span>class <span class="ident">AverageMeter</span></span>
</code></dt>
<dd>
<div class="desc"><p>Computes and stores the average and current value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L53-L69" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class AverageMeter(object):
    &#34;&#34;&#34;Computes and stores the average and current value&#34;&#34;&#34;

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="openunmix.utils.AverageMeter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L59-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def reset(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0</code></pre>
</details>
</dd>
<dt id="openunmix.utils.AverageMeter.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, val, n=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L65-L69" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="openunmix.utils.EarlyStopping"><code class="flex name class">
<span>class <span class="ident">EarlyStopping</span></span>
<span>(</span><span>mode='min', min_delta=0, patience=10)</span>
</code></dt>
<dd>
<div class="desc"><p>Early Stopping Monitor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L72-L112" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class EarlyStopping(object):
    &#34;&#34;&#34;Early Stopping Monitor&#34;&#34;&#34;

    def __init__(self, mode=&#34;min&#34;, min_delta=0, patience=10):
        self.mode = mode
        self.min_delta = min_delta
        self.patience = patience
        self.best = None
        self.num_bad_epochs = 0
        self.is_better = None
        self._init_is_better(mode, min_delta)

        if patience == 0:
            self.is_better = lambda a, b: True

    def step(self, metrics):
        if self.best is None:
            self.best = metrics
            return False

        if np.isnan(metrics):
            return True

        if self.is_better(metrics, self.best):
            self.num_bad_epochs = 0
            self.best = metrics
        else:
            self.num_bad_epochs += 1

        if self.num_bad_epochs &gt;= self.patience:
            return True

        return False

    def _init_is_better(self, mode, min_delta):
        if mode not in {&#34;min&#34;, &#34;max&#34;}:
            raise ValueError(&#34;mode &#34; + mode + &#34; is unknown!&#34;)
        if mode == &#34;min&#34;:
            self.is_better = lambda a, best: a &lt; best - min_delta
        if mode == &#34;max&#34;:
            self.is_better = lambda a, best: a &gt; best + min_delta</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="openunmix.utils.EarlyStopping.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, metrics)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/sigsep/open-unmix-pytorch/blob/b436d5f7d40c2b8ff0b2500e9d953fa47929b261/openunmix/utils.py#L87-L104" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def step(self, metrics):
    if self.best is None:
        self.best = metrics
        return False

    if np.isnan(metrics):
        return True

    if self.is_better(metrics, self.best):
        self.num_bad_epochs = 0
        self.best = metrics
    else:
        self.num_bad_epochs += 1

    if self.num_bad_epochs &gt;= self.patience:
        return True

    return False</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="openunmix" href="index.html">openunmix</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="openunmix.utils.bandwidth_to_max_bin" href="#openunmix.utils.bandwidth_to_max_bin">bandwidth_to_max_bin</a></code></li>
<li><code><a title="openunmix.utils.load_separator" href="#openunmix.utils.load_separator">load_separator</a></code></li>
<li><code><a title="openunmix.utils.load_target_models" href="#openunmix.utils.load_target_models">load_target_models</a></code></li>
<li><code><a title="openunmix.utils.preprocess" href="#openunmix.utils.preprocess">preprocess</a></code></li>
<li><code><a title="openunmix.utils.save_checkpoint" href="#openunmix.utils.save_checkpoint">save_checkpoint</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="openunmix.utils.AverageMeter" href="#openunmix.utils.AverageMeter">AverageMeter</a></code></h4>
<ul class="">
<li><code><a title="openunmix.utils.AverageMeter.reset" href="#openunmix.utils.AverageMeter.reset">reset</a></code></li>
<li><code><a title="openunmix.utils.AverageMeter.update" href="#openunmix.utils.AverageMeter.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="openunmix.utils.EarlyStopping" href="#openunmix.utils.EarlyStopping">EarlyStopping</a></code></h4>
<ul class="">
<li><code><a title="openunmix.utils.EarlyStopping.step" href="#openunmix.utils.EarlyStopping.step">step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>